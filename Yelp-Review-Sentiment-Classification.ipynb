{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2jS5ThMCEvnC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import wordcloud\n",
    "import os \n",
    "import sys\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "!python -m spacy download en_core_web_md\n",
    "import en_core_web_md\n",
    "text_to_nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KsF8tioZLicU"
   },
   "outputs": [],
   "source": [
    "data_file  = 'yelp_final.csv'\n",
    "\n",
    "!wget https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%203%20-%20NLP/yelp_final.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dZ_lymcN_K9"
   },
   "outputs": [],
   "source": [
    "yelp_full = pd.read_csv(data_file)\n",
    "yelp_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pB1yKcUtcpg9"
   },
   "outputs": [],
   "source": [
    "needed_columns = [] \n",
    "yelp = yelp_full[needed_columns]\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "8T93-bsPHalS"
   },
   "outputs": [],
   "source": [
    "needed_columns = ['stars','text']\n",
    "yelp = yelp_full[needed_columns]\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "la3rUPKgEvoi"
   },
   "outputs": [],
   "source": [
    "num_stars =  1\n",
    "\n",
    "for t in yelp[yelp['stars'] == num_stars]['text'].head(20).values:\n",
    "    print (t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FtMnf1zLEvo_"
   },
   "outputs": [],
   "source": [
    "num_stars =  1\n",
    "this_star_text = ''\n",
    "for t in yelp[yelp['stars'] == num_stars]['text'].values: \n",
    "    this_star_text += t + ' '\n",
    "    \n",
    "wordcloud = WordCloud()    \n",
    "wordcloud.generate_from_text(this_star_text)\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "x2uO1pbAEvrI"
   },
   "outputs": [],
   "source": [
    "def is_good_review(num_stars):\n",
    "    if num_stars > 3: \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "yelp['is_good_review'] = yelp['stars'].apply(is_good_review)\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7XOaa1uEEvpY"
   },
   "outputs": [],
   "source": [
    "example_text = \"All the people I spoke to were super nice and very welcoming.\" #@param {type:\"string\"}\n",
    "tokens = word_tokenize(example_text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqq4w-ZrEvpj"
   },
   "outputs": [],
   "source": [
    "example_word = \"not\" \n",
    "if example_word.lower() in STOP_WORDS:\n",
    "  print ('\"' + example_word + '\" is a stop word.')\n",
    "else:\n",
    "  print ('\"' + example_word + '\" is NOT a stop word.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t6HQm1vEvrQ"
   },
   "outputs": [],
   "source": [
    "X_text = yelp['text']\n",
    "y = yelp['is_good_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrSQAeKjAiXJ"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    clean_tokens = []\n",
    "    for token in text_to_nlp(text):\n",
    "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): # -PRON- is a special all inclusive \"lemma\" spaCy uses for any pronoun, we want to exclude these \n",
    "            clean_tokens.append(token.lemma_)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blZ7RJ2zEvrU"
   },
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=tokenize, max_features=800).fit(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TjdgYVxmKgd"
   },
   "outputs": [],
   "source": [
    "bow_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upN1gxm5Evrb"
   },
   "outputs": [],
   "source": [
    "len(bow_transformer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRJJe2HGEvr6"
   },
   "outputs": [],
   "source": [
    "X = bow_transformer.transform(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdeKo8ZQTiOu"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PThy6pNUEvsA"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vQNdY8k8X6bq"
   },
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "E_0C2bHaV84F"
   },
   "outputs": [],
   "source": [
    "y_pred = logistic_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "euuR1VWWEvsX"
   },
   "outputs": [],
   "source": [
    "example_review = \"This restaurant sucks\" \n",
    "prediction = logistic_model.predict(bow_transformer.transform([example_review]))\n",
    "\n",
    "if prediction:\n",
    "  print (\"This was a GOOD review!\")\n",
    "else:\n",
    "  print (\"This was a BAD review!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4qISnUEcmumt"
   },
   "outputs": [],
   "source": [
    "stakeholders = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPg0Y7cjHH2c"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "uDjdKj3RIoD8"
   },
   "outputs": [],
   "source": [
    "nb_model.fit(X_train, y_train)\n",
    "nb_preds = nb_model.predict(X_test) \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print (\"The accuracy of the model is \" + str(accuracy*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nuu0QFg-VOiN"
   },
   "outputs": [],
   "source": [
    "text_to_nlp = en_core_web_md.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldEPkz8NcI6_"
   },
   "outputs": [],
   "source": [
    "doc = text_to_nlp(\"I like apples and cherries and peaches and pie\")\n",
    "token = doc[2] \n",
    "print (token)\n",
    "print (len(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0tBmojQeNhn"
   },
   "outputs": [],
   "source": [
    "print ('Vector for: ', token)\n",
    "print (token.vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZD8fVPwlM4Bn"
   },
   "outputs": [],
   "source": [
    "doc = text_to_nlp(u\"guitar and piano\")\n",
    "word1 = doc[0]\n",
    "word2 = doc[2]\n",
    "word1.similarity(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAOs86fEQ_2l"
   },
   "outputs": [],
   "source": [
    "def tokenize_vecs(text):\n",
    "    clean_tokens = []\n",
    "    for token in text_to_nlp(text):\n",
    "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): \n",
    "            clean_tokens.append(token)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "URKBtiv7cmUG"
   },
   "outputs": [],
   "source": [
    "X_word2vec = []\n",
    "for text in X_text:\n",
    "  review = tokenize_vecs(text) \n",
    "  review_vec = [0]*300\n",
    "  for word in review:\n",
    "    review_vec += word.vector\n",
    "  review_vec = review_vec / len(review)\n",
    "  X_word2vec.append(review_vec)\n",
    "\n",
    "X_word2vec = np.array(X_word2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "IzEOX1q9GYSl"
   },
   "outputs": [],
   "source": [
    "w2v_model = LogisticRegression()\n",
    "\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(X_word2vec, y, test_size=0.2, random_state=101)\n",
    "\n",
    "w2v_model.fit(X_train_word2vec, y_train_word2vec)\n",
    "\n",
    "w2v_preds = w2v_model.predict(X_test_word2vec) \n",
    "accuracy = accuracy_score(y_test_word2vec, w2v_preds)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHlGBkBKoUPI"
   },
   "outputs": [],
   "source": [
    "vocab_dict = dict() \n",
    "\n",
    "for word in bow_transformer.vocabulary_:\n",
    "    vocab_dict[word] = text_to_nlp(word).vector \n",
    "\n",
    "for word, vec in vocab_dict.items(): \n",
    "  print ('Word: {}. Vector length: {}'.format(word, len(vec)))\n",
    "\n",
    "print()\n",
    "print ('{} words in our dictionary'.format(len(vocab_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omAmAv88GZUp"
   },
   "outputs": [],
   "source": [
    "v0 = [2,3,1]\n",
    "v1 = [2,4,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "QtbbBLgcFmE0"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def vector_plot(tvects,is_vect=True,orig=[0,0,0]):\n",
    "    \"\"\"Plot vectors using plotly\"\"\"\n",
    "\n",
    "    if is_vect:\n",
    "        if not hasattr(orig[0],\"__iter__\"):\n",
    "            coords = [[orig,np.sum([orig,v],axis=0)] for v in tvects]\n",
    "        else:\n",
    "            coords = [[o,np.sum([o,v],axis=0)] for o,v in zip(orig,tvects)]\n",
    "    else:\n",
    "        coords = tvects\n",
    "\n",
    "    data = []\n",
    "    for i,c in enumerate(coords):\n",
    "        X1, Y1, Z1 = zip(c[0])\n",
    "        X2, Y2, Z2 = zip(c[1])\n",
    "        vector = go.Scatter3d(x = [X1[0],X2[0]],\n",
    "                              y = [Y1[0],Y2[0]],\n",
    "                              z = [Z1[0],Z2[0]],\n",
    "                              marker = dict(size = [0,5],\n",
    "                                            color = ['blue'],\n",
    "                                            line=dict(width=5,\n",
    "                                                      color='DarkSlateGrey')),\n",
    "                              name = 'Vector'+str(i+1))\n",
    "        data.append(vector)\n",
    "\n",
    "    layout = go.Layout(\n",
    "             margin = dict(l = 4,\n",
    "                           r = 4,\n",
    "                           b = 4,\n",
    "                           t = 4)\n",
    "                  )\n",
    "    fig = go.Figure(data=data,layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "vector_plot([v0,v1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nPf7SLCGJtBZ"
   },
   "outputs": [],
   "source": [
    "def vector_cosine_similarity(vec1,vec2):\n",
    "\n",
    "  numerator = 0\n",
    "  for i in range(len(vec1)):\n",
    "    numerator += vec1[i]*vec2[i]\n",
    "  mag1 = (sum(elem**2 for elem in vec1))**0.5\n",
    "  mag2 = (sum(elem**2 for elem in vec2))**0.5\n",
    "  similarity = numerator/(mag1*mag2)\n",
    "  return similarity\n",
    "\n",
    "print(vector_cosine_similarity(v0,v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NmdssfuLLhvx"
   },
   "outputs": [],
   "source": [
    "def word_similarity(word1, word2):\n",
    "  \n",
    "  try:\n",
    "    vec1 = vocab_dict[word1]\n",
    "    vec2 = vocab_dict[word2]\n",
    "    return vector_cosine_similarity(vec1,vec2)\n",
    "\n",
    "  except KeyError:\n",
    "    print ('Word not in dictionary')\n",
    "\n",
    "print(word_similarity('burger','steak'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4AgwsvmZOmdd"
   },
   "outputs": [],
   "source": [
    "def find_nearest_neighbor(word):\n",
    "  try:\n",
    "    vec = vocab_dict[word]\n",
    "    find_most_similar(vec)\n",
    "  except KeyError:\n",
    "    print ('Word not in dictionary')\n",
    "\n",
    "def find_most_similar(start_vec):\n",
    " \n",
    "  similarity_series = pd.Series(np.nan, index = vocab_dict.keys())\n",
    "  for word, vec in vocab_dict.items():\n",
    "    similarity_series[word] = vector_cosine_similarity(start_vec, vec)\n",
    "  similarity_series = similarity_series[similarity_series.notna()]  \n",
    "  five_most_similar = similarity_series.sort_values().tail()\n",
    "  print (five_most_similar) \n",
    "\n",
    "find_nearest_neighbor('bagel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "gr88Et0_Q55g"
   },
   "outputs": [],
   "source": [
    "def find_analogy(word_a1, word_a2, word_b1):\n",
    "  \n",
    "  a1_vec = vocab_dict[word_a1]\n",
    "  a2_vec = vocab_dict[word_a2]\n",
    "  b1_vec = vocab_dict[word_b1]\n",
    "  find_most_similar(b1_vec - a1_vec + a2_vec)\n",
    "\n",
    "find_analogy('breakfast','bagel','lunch')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "6mLW7krRcBIG",
    "JBagECaVMQXZ"
   ],
   "name": "Copy of Yelp_Review_Sentiment_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
